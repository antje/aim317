{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN THIS NOTEBOOK UNTIL THE PREVIOUS NOTEBOOK FINISHES COMPLETELY \n",
    "\n",
    "# THE PREVIOUS NOTEBOOK WILL TAKE 20-30 MINUTES.\n",
    "\n",
    "# PLEASE BE PATIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster status:  ACTIVE.  Please continue.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "AWS_CLUSTER_STATUS=$(aws eks describe-cluster --name ${AWS_CLUSTER_NAME} --region ${AWS_REGION} --query \"cluster.status\" --output text)\n",
    "\n",
    "if [ -n \"$AWS_CLUSTER_STATUS\" ]; then\n",
    "    if [ $AWS_CLUSTER_STATUS == \"ACTIVE\" ]; then\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please continue.\"\n",
    "    else\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "        exit\n",
    "    fi\n",
    "else\n",
    "  echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "  exit\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT CONTINUE UNTIL THE CLUSTER STATUS IS `ACTIVE` ^^ ABOVE ^^."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate IAM Policies with EKS Worker Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster status:  ACTIVE.\n",
      "eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "arn:aws:iam::231218423789:role/eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "INSTANCE_ROLE_NAME:  eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "export INSTANCE_ROLE_NAME=eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "INSTANCE_PROFILE_ARN:  arn:aws:iam::231218423789:role/eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "export INSTANCE_PROFILE_ARN=arn:aws:iam::231218423789:role/eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "    \n",
    "AWS_CLUSTER_STATUS=$(aws eks describe-cluster --name ${AWS_CLUSTER_NAME} --region ${AWS_REGION} --query \"cluster.status\" --output text)\n",
    "\n",
    "if [ -n \"$AWS_CLUSTER_STATUS\" ]; then\n",
    "    if [ $AWS_CLUSTER_STATUS == \"ACTIVE\" ]; then\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.\"\n",
    "        aws iam list-roles \\\n",
    "            | jq -r \".Roles[] \\\n",
    "            | select(.RoleName \\\n",
    "            | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "            .RoleName\"\n",
    "\n",
    "        aws iam list-roles \\\n",
    "            | jq -r \".Roles[] \\\n",
    "            | select(.RoleName \\\n",
    "            | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "            .Arn\" \n",
    "\n",
    "        export INSTANCE_ROLE_NAME=$(aws iam list-roles \\\n",
    "            | jq -r \".Roles[] \\\n",
    "            | select(.RoleName \\\n",
    "            | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "            .RoleName\")\n",
    "        if [ -n \"$INSTANCE_ROLE_NAME\" ]; then\n",
    "            echo \"INSTANCE_ROLE_NAME:  $INSTANCE_ROLE_NAME\"\n",
    "            echo \"export INSTANCE_ROLE_NAME=${INSTANCE_ROLE_NAME}\" | tee -a ~/.bash_profile\n",
    "        else\n",
    "            echo \"Please re-run this notebook after cluster status:  ACTIVE.\"\n",
    "        fi\n",
    "\n",
    "        export INSTANCE_PROFILE_ARN=$(aws iam list-roles \\\n",
    "            | jq -r \".Roles[] \\\n",
    "            | select(.RoleName \\\n",
    "            | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "            .Arn\")\n",
    "        if [ -n \"$INSTANCE_PROFILE_ARN\" ]; then\n",
    "            echo \"INSTANCE_PROFILE_ARN:  $INSTANCE_PROFILE_ARN\"\n",
    "            echo \"export INSTANCE_PROFILE_ARN=${INSTANCE_PROFILE_ARN}\" | tee -a ~/.bash_profile\n",
    "        else\n",
    "            echo \"Please re-run this notebook after cluster status:  ACTIVE.\"\n",
    "        fi\n",
    "    else\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "        exit\n",
    "    fi\n",
    "else\n",
    "  echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "  exit\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Roles\n",
    "* Allow Access from/to the Elastic Container Registry (ECR)\n",
    "* This allows our cluster worker nodes to load custom Docker images (ie. models) from ECR. \n",
    "* We will load these custom Docker images in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTANCE_ROLE_NAME:  eksctl-workshop-nodegroup-ng-9706-NodeInstanceRole-XVPKFVUBE1J2\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source ~/.bash_profile\n",
    "\n",
    "if [ -n \"$INSTANCE_ROLE_NAME\" ]; then\n",
    "    echo \"INSTANCE_ROLE_NAME:  $INSTANCE_ROLE_NAME\"\n",
    "    aws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\n",
    "    echo \"Completed\"\n",
    "else\n",
    "    echo \"Please re-run this notebook when the EKS cluster status is ACTIVE.\"\n",
    "    exit\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate IAM and OIDC\n",
    "To use IAM Roles for Service Accounts in your cluster, you must create an OIDC identity provider in the IAM console.  \n",
    "\n",
    "See https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html for more info.\n",
    "\n",
    "# THIS WILL TAKE 5-10 MINUTES.  PLEASE BE PATIENT.\n",
    "\n",
    "# _If you see `retryable error`s below, this is OK!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster status:  ACTIVE.\n",
      "[ℹ]  eksctl version 0.32.0\n",
      "[ℹ]  using region us-west-2\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 47.318199ms\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 83.381544ms\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 139.735496ms\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 347.3598ms\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 870.878832ms\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 1.56046608s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 2.925631936s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 4.161712768s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 15.080168448s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 21.50802432s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 35.254550528s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 1m41.162375168s\n",
      "[!]  retryable error (RequestError: send request failed\n",
      "caused by: Put \"http://169.254.169.254/latest/api/token\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)) from ec2metadata/GetToken - will retry after delay of 2m32.075681792s\n",
      "request expired, resigning\n",
      "[ℹ]  will create IAM Open ID Connect provider for cluster \"workshop\" in \"us-west-2\"\n",
      "[✔]  created IAM Open ID Connect provider for cluster \"workshop\" in \"us-west-2\"\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "AWS_CLUSTER_STATUS=$(aws eks describe-cluster --name ${AWS_CLUSTER_NAME} --region ${AWS_REGION} --query \"cluster.status\" --output text)\n",
    "\n",
    "if [ -n \"$AWS_CLUSTER_STATUS\" ]; then\n",
    "    if [ $AWS_CLUSTER_STATUS == \"ACTIVE\" ]; then\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.\"\n",
    "        eksctl utils associate-iam-oidc-provider --cluster ${AWS_CLUSTER_NAME} --approve\n",
    "        echo \"Completed\"\n",
    "    else\n",
    "        echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "        exit\n",
    "    fi\n",
    "else\n",
    "  echo \"Cluster status:  $AWS_CLUSTER_STATUS.  Please wait for status:  Created\"\n",
    "  exit\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _If you see `retryable error`s ^^ above ^^, this is OK!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_OIDC_ISSUER:  https://oidc.eks.us-west-2.amazonaws.com/id/B4BF986DB5B5690296C50C54CAF37132\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "AWS_OIDC_ISSUER=$(aws eks describe-cluster --name ${AWS_CLUSTER_NAME} --region ${AWS_REGION} --query \"cluster.identity.oidc.issuer\" --output text)\n",
    "if [ -n \"$AWS_OIDC_ISSUER\" ]; then\n",
    "    echo \"AWS_OIDC_ISSUER:  $AWS_OIDC_ISSUER\"\n",
    "    echo \"Completed\"\n",
    "else\n",
    "    echo \"AWS_OIDC_ISSUER:  $AWS_OIDC_ISSUER\"\n",
    "    echo \"Something is wrong.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update `~/.kube/config` with our new EKS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new context arn:aws:eks:us-west-2:231218423789:cluster/workshop to /home/ec2-user/.kube/config\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source ~/.bash_profile\n",
    "\n",
    "aws eks --region ${AWS_REGION} update-kubeconfig --name ${AWS_CLUSTER_NAME} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Your EKS Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              STATUS   AGE\n",
      "default           Active   16m\n",
      "kube-node-lease   Active   16m\n",
      "kube-public       Active   16m\n",
      "kube-system       Active   16m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl get ns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                           STATUS   ROLES    AGE     VERSION\n",
      "ip-192-168-18-117.us-west-2.compute.internal   Ready    <none>   8m42s   v1.18.9-eks-d1db3c\n",
      "ip-192-168-30-41.us-west-2.compute.internal    Ready    <none>   8m41s   v1.18.9-eks-d1db3c\n",
      "ip-192-168-48-83.us-west-2.compute.internal    Ready    <none>   8m46s   v1.18.9-eks-d1db3c\n",
      "ip-192-168-50-254.us-west-2.compute.internal   Ready    <none>   8m41s   v1.18.9-eks-d1db3c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl get nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
