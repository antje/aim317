{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a BERT Model and Create a Text Classifier\n",
    "\n",
    "In the previous section, we've already performed the Feature Engineering to create BERT embeddings from the `reviews_body` text using the pre-trained BERT model, and split the dataset into train, validation and test files. To optimize for Tensorflow training, we saved the files in TFRecord format. \n",
    "\n",
    "Now, let’s fine-tune the BERT model to our Customer Reviews Dataset and add a new classification layer to predict the `star_rating` for a given `review_body`.\n",
    "\n",
    "![BERT Training](img/bert_training.png)\n",
    "\n",
    "As mentioned earlier, BERT’s attention mechanism is called a Transformer. This is, not coincidentally, the name of the popular BERT Python library, “Transformers,” maintained by a company called HuggingFace. \n",
    "\n",
    "We will use a variant of BERT called [**DistilBert**](https://arxiv.org/pdf/1910.01108.pdf) which requires less memory and compute, but maintains very good accuracy on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.configuration_distilbert import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    max_seq_length\n",
    "except NameError:\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the PREPARE section before you continue.')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_dataset_builder(channel,\n",
    "                                     input_filenames,\n",
    "                                     pipe_mode,\n",
    "                                     is_training,\n",
    "                                     drop_remainder):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print('***** Using pipe_mode with channel {}'.format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "        dataset = PipeModeDataset(channel=channel,\n",
    "                                  record_format='TFRecord')\n",
    "    else:\n",
    "        print('***** Using input_filenames {}'.format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(100)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        return tf.io.parse_single_example(record, name_to_features)\n",
    "        \n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "          lambda record: _decode_record(record, name_to_features),\n",
    "          batch_size=8,\n",
    "          drop_remainder=drop_remainder,\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(seed=42,\n",
    "                                  buffer_size=10,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data-tfrecord/bert-train'\n",
    "train_data_filenames = glob('{}/*.tfrecord'.format(train_data))\n",
    "print('train_data_filenames {}'.format(train_data_filenames))\n",
    "\n",
    "train_dataset = file_based_input_dataset_builder(\n",
    "    channel='train',\n",
    "    input_filenames=train_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=True,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = './data-tfrecord/bert-validation'\n",
    "validation_data_filenames = glob('{}/*.tfrecord'.format(validation_data))\n",
    "print('validation_data_filenames {}'.format(validation_data_filenames))\n",
    "\n",
    "validation_dataset = file_based_input_dataset_builder(\n",
    "    channel='validation',\n",
    "    input_filenames=validation_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = './data-tfrecord/bert-test'\n",
    "test_data_filenames = glob('{}/*.tfrecord'.format(test_data))\n",
    "print(test_data_filenames)\n",
    "\n",
    "test_dataset = file_based_input_dataset_builder(\n",
    "    channel='test',\n",
    "    input_filenames=test_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Manual Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "steps_per_epoch=50\n",
    "validation_steps=50\n",
    "test_steps=50\n",
    "freeze_bert_layer=True\n",
    "learning_rate=3e-5\n",
    "epsilon=1e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained BERT Model \n",
    "https://huggingface.co/transformers/pretrained_models.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=[1, 2, 3, 4, 5]\n",
    "\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\n",
    "                                          num_labels=len(CLASSES))\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
    "                                                              config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Custom Classifier Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric=tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.layers[0].trainable=not freeze_bert_layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "log_dir = './tmp/tensorboard/'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    shuffle=True,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained model {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Holdout Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = model.evaluate(test_dataset,\n",
    "                              steps=test_steps,                            \n",
    "                              callbacks=callbacks)\n",
    "print(test_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './tmp/fine-tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $model_dir/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_dir,\n",
    "                                                                     id2label={\n",
    "                                                                       0: 1,\n",
    "                                                                       1: 2,\n",
    "                                                                       2: 3,\n",
    "                                                                       3: 4,\n",
    "                                                                       4: 5\n",
    "                                                                     },\n",
    "                                                                     label2id={\n",
    "                                                                       1: 0,\n",
    "                                                                       2: 1,\n",
    "                                                                       3: 2,\n",
    "                                                                       4: 3,\n",
    "                                                                       5: 4\n",
    "                                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='tf',\n",
    "                                                device=-1) # -1 is CPU, >= 0 is GPU\n",
    "\n",
    "print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\n",
    "print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df_sample_reviews = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \n",
    "                                delimiter='\\t', \n",
    "                                quoting=csv.QUOTE_NONE,\n",
    "                                compression='gzip')[['review_body', 'star_rating']].sample(n=100)\n",
    "df_sample_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict(review_body):\n",
    "    prediction_map = inference_pipeline(review_body)\n",
    "    return prediction_map[0]['label']\n",
    "    \n",
    "y_pred = df_sample_reviews['review_body'].map(predict)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = df_sample_reviews['star_rating']\n",
    "\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(y_pred=y_pred, y_true=y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "def plot_conf_mat(cm, classes, title, cmap = plt.cm.Greens):\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plot_conf_mat(cm, \n",
    "              classes=['1', '2', '3', '4', '5'], \n",
    "              title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
