{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy BERT Model Training Pod\n",
    "\n",
    "https://github.com/kubernetes-sigs/aws-fsx-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive s3://fsx-antje/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m \n",
      "\u001b[94mapiVersion\u001b[39;49;00m: v1\n",
      "\u001b[94mkind\u001b[39;49;00m: Pod\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: bert-model-training\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mvolumes\u001b[39;49;00m:\n",
      "  - \u001b[94mname\u001b[39;49;00m: fsx-opt-ml\n",
      "    \u001b[94mpersistentVolumeClaim\u001b[39;49;00m:\n",
      "      \u001b[94mclaimName\u001b[39;49;00m: fsx-claim\n",
      "  \u001b[94mcontainers\u001b[39;49;00m: \n",
      "    - \u001b[94mname\u001b[39;49;00m: bert\n",
      "      \u001b[94mcommand\u001b[39;49;00m: \n",
      "        - python\n",
      "        - /opt/ml/code/train.py\n",
      "        - --train_steps_per_epoch=1\n",
      "        - --epochs=1\n",
      "        - --learning_rate=0.00001\n",
      "        - --epsilon=0.00000001\n",
      "        - --train_batch_size=36\n",
      "        - --validation_batch_size=18\n",
      "        - --test_batch_size=18\n",
      "        - --train_steps_per_epoch=1\n",
      "        - --validation_steps=1\n",
      "        - --test_steps=1\n",
      "        - --use_xla=True\n",
      "        - --use_amp=False\n",
      "        - --max_seq_length=64\n",
      "        - --freeze_bert_layer=True\n",
      "        - --enable_sagemaker_debugger=False\n",
      "        - --enable_checkpointing=False\n",
      "        - --enable_tensorboard=False\n",
      "        - --run_validation=True\n",
      "        - --run_test=False\n",
      "        - --run_sample_predictions=False\n",
      "      \u001b[94mimage\u001b[39;49;00m: 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\n",
      "      \u001b[94mimagePullPolicy\u001b[39;49;00m: Always\n",
      "      \u001b[94menv\u001b[39;49;00m: \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_TRAINING_ENV\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33m:true}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SAGEMAKER_JOB_NAME\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mtf-bert-training-eks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CURRENT_HOST\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mlocalhost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_NUM_GPUS\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_HOSTS\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33mlocalhost\u001b[39;49;00m\u001b[33m\\\"\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_MODEL_DIR\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m     \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_OUTPUT_DIR\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_OUTPUT_DATA_DIR\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_TRAIN\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_VALIDATION\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m     \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_TEST\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: HDF5_USE_FILE_LOCKING\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mFALSE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "      \u001b[94msecurityContext\u001b[39;49;00m:\n",
      "        \u001b[94mprivileged\u001b[39;49;00m: true\n",
      "      \u001b[94mvolumeMounts\u001b[39;49;00m:\n",
      "      - \u001b[94mmountPath\u001b[39;49;00m: /opt/ml/\n",
      "        \u001b[94mname\u001b[39;49;00m: fsx-opt-ml\n",
      "  \u001b[94mrestartPolicy\u001b[39;49;00m: Never \n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/bert-model-training created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: the path \"bert-csi-fsx.yaml\" does not exist\n"
     ]
    }
   ],
   "source": [
    "#!kubectl delete -f train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  READY   STATUS    RESTARTS   AGE\n",
      "bert-model-training   1/1     Running   0          16s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod bert-model-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         bert-model-training\n",
      "Namespace:    default\n",
      "Priority:     0\n",
      "Node:         ip-192-168-25-72.us-west-2.compute.internal/192.168.25.72\n",
      "Start Time:   Mon, 02 Nov 2020 12:12:40 +0000\n",
      "Labels:       <none>\n",
      "Annotations:  kubernetes.io/psp: eks.privileged\n",
      "Status:       Running\n",
      "IP:           192.168.9.68\n",
      "IPs:\n",
      "  IP:  192.168.9.68\n",
      "Containers:\n",
      "  bert:\n",
      "    Container ID:  docker://8512e1427d631143cf06e666f7e49f3d66c15e1aa41c29d1c32a6cd0f39c522e\n",
      "    Image:         763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\n",
      "    Image ID:      docker-pullable://763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training@sha256:4911ac31a130c68a2f92b72dd81d22bd02b542cc549c5652f22c1f24e702eaf5\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      python\n",
      "      /opt/ml/code/train.py\n",
      "      --train_steps_per_epoch=1\n",
      "      --epochs=1\n",
      "      --learning_rate=0.00001\n",
      "      --epsilon=0.00000001\n",
      "      --train_batch_size=36\n",
      "      --validation_batch_size=18\n",
      "      --test_batch_size=18\n",
      "      --train_steps_per_epoch=1\n",
      "      --validation_steps=1\n",
      "      --test_steps=1\n",
      "      --use_xla=True\n",
      "      --use_amp=False\n",
      "      --max_seq_length=64\n",
      "      --freeze_bert_layer=True\n",
      "      --enable_sagemaker_debugger=False\n",
      "      --enable_checkpointing=False\n",
      "      --enable_tensorboard=False\n",
      "      --run_validation=True\n",
      "      --run_test=False\n",
      "      --run_sample_predictions=False\n",
      "    State:          Running\n",
      "      Started:      Mon, 02 Nov 2020 12:12:42 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      SM_TRAINING_ENV:        {\"is_master\":true}\n",
      "      SAGEMAKER_JOB_NAME:     tf-bert-training-eks\n",
      "      SM_CURRENT_HOST:        localhost\n",
      "      SM_NUM_GPUS:            0\n",
      "      SM_HOSTS:               {\"hosts\":\"localhost\"}\n",
      "      SM_MODEL_DIR:           /opt/ml/model/\n",
      "      SM_OUTPUT_DIR:          /opt/ml/output/\n",
      "      SM_OUTPUT_DATA_DIR:     /opt/ml/output/data/\n",
      "      SM_CHANNEL_TRAIN:       /opt/ml/input/data/train\n",
      "      SM_CHANNEL_VALIDATION:  /opt/ml/input/data/validation\n",
      "      SM_CHANNEL_TEST:        /opt/ml/input/data/test\n",
      "      HDF5_USE_FILE_LOCKING:  FALSE\n",
      "    Mounts:\n",
      "      /opt/ml/ from fsx-opt-ml (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrfzd (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  fsx-opt-ml:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  fsx-claim\n",
      "    ReadOnly:   false\n",
      "  default-token-zrfzd:\n",
      "    Type:        Secret (a volume populated by a Secret)\n",
      "    SecretName:  default-token-zrfzd\n",
      "    Optional:    false\n",
      "QoS Class:       BestEffort\n",
      "Node-Selectors:  <none>\n",
      "Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n",
      "                 node.kubernetes.io/unreachable:NoExecute for 300s\n",
      "Events:\n",
      "  Type    Reason     Age   From                                                  Message\n",
      "  ----    ------     ----  ----                                                  -------\n",
      "  Normal  Scheduled  19s   default-scheduler                                     Successfully assigned default/bert-model-training to ip-192-168-25-72.us-west-2.compute.internal\n",
      "  Normal  Pulling    18s   kubelet, ip-192-168-25-72.us-west-2.compute.internal  Pulling image \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\"\n",
      "  Normal  Pulled     18s   kubelet, ip-192-168-25-72.us-west-2.compute.internal  Successfully pulled image \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\"\n",
      "  Normal  Created    18s   kubelet, ip-192-168-25-72.us-west-2.compute.internal  Created container bert\n",
      "  Normal  Started    17s   kubelet, ip-192-168-25-72.us-west-2.compute.internal  Started container bert\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod bert-model-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.10.28-cp36-cp36m-manylinux2010_x86_64.whl (666 kB)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.43)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.22.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.43)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.25.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers==2.8.0) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=ff9d85d5019d0358d942c64f28ed7f25a512d5044038b747948558474b38dcda\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, filelock, tqdm, regex, sacremoses, sentencepiece, dataclasses, transformers\n",
      "Successfully installed dataclasses-0.7 filelock-3.0.12 regex-2020.10.28 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 tqdm-4.51.0 transformers-2.8.0\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Requirement already satisfied: sagemaker-tensorflow==2.1.0.1.0.0 in /usr/local/lib/python3.6/dist-packages (2.1.0.1.0.0)\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Collecting smdebug==0.9.3\n",
      "  Downloading smdebug-0.9.3-py2.py3-none-any.whl (174 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from smdebug==0.9.3) (20.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from smdebug==0.9.3) (3.11.3)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /usr/local/lib/python3.6/dist-packages (from smdebug==0.9.3) (1.12.43)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from smdebug==0.9.3) (1.18.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->smdebug==0.9.3) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->smdebug==0.9.3) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->smdebug==0.9.3) (46.1.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3>=1.10.32->smdebug==0.9.3) (1.15.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3>=1.10.32->smdebug==0.9.3) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3>=1.10.32->smdebug==0.9.3) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3>=1.10.32->smdebug==0.9.3) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3>=1.10.32->smdebug==0.9.3) (0.15.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3>=1.10.32->smdebug==0.9.3) (1.25.9)\n",
      "Installing collected packages: smdebug\n",
      "  Attempting uninstall: smdebug\n",
      "    Found existing installation: smdebug 0.7.2\n",
      "    Uninstalling smdebug-0.7.2:\n",
      "      Successfully uninstalled smdebug-0.7.2\n",
      "Successfully installed smdebug-0.9.3\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Collecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22\n",
      "    Uninstalling scikit-learn-0.22:\n",
      "      Successfully uninstalled scikit-learn-0.22\n",
      "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.1) (1.14.0)\n",
      "Installing collected packages: cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.2.1\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Environment Variables:\n",
      "{'DEBCONF_NONINTERACTIVE_SEEN': 'true',\n",
      " 'DEBIAN_FRONTEND': 'noninteractive',\n",
      " 'HDF5_USE_FILE_LOCKING': 'FALSE',\n",
      " 'HOME': '/root',\n",
      " 'HOSTNAME': 'bert-model-training',\n",
      " 'KMP_AFFINITY': 'granularity=fine,compact,1,0',\n",
      " 'KMP_BLOCKTIME': '1',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'True',\n",
      " 'KMP_INIT_AT_FORK': 'FALSE',\n",
      " 'KMP_SETTINGS': '0',\n",
      " 'KUBERNETES_PORT': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP_ADDR': '10.100.0.1',\n",
      " 'KUBERNETES_PORT_443_TCP_PORT': '443',\n",
      " 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp',\n",
      " 'KUBERNETES_SERVICE_HOST': '10.100.0.1',\n",
      " 'KUBERNETES_SERVICE_PORT': '443',\n",
      " 'KUBERNETES_SERVICE_PORT_HTTPS': '443',\n",
      " 'LANG': 'C.UTF-8',\n",
      " 'LC_ALL': 'C.UTF-8',\n",
      " 'LD_LIBRARY_PATH': '/usr/local/openmpi/lib:',\n",
      " 'PATH': '/usr/local/openmpi/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
      " 'PYTHONDONTWRITEBYTECODE': '1',\n",
      " 'PYTHONIOENCODING': 'UTF-8',\n",
      " 'PYTHONUNBUFFERED': '1',\n",
      " 'SAGEMAKER_JOB_NAME': 'tf-bert-training-eks',\n",
      " 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_tensorflow_container.training:main',\n",
      " 'SM_CHANNEL_TEST': '/opt/ml/input/data/test',\n",
      " 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\n",
      " 'SM_CHANNEL_VALIDATION': '/opt/ml/input/data/validation',\n",
      " 'SM_CURRENT_HOST': 'localhost',\n",
      " 'SM_HOSTS': '{\"hosts\":\"localhost\"}',\n",
      " 'SM_MODEL_DIR': '/opt/ml/model/',\n",
      " 'SM_NUM_GPUS': '0',\n",
      " 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data/',\n",
      " 'SM_OUTPUT_DIR': '/opt/ml/output/',\n",
      " 'SM_TRAINING_ENV': '{\"is_master\":true}'}\n",
      "Listing /opt...\n",
      "/opt/ml/model/tensorflow/saved_model/0,['assets', 'variables'],saved_model.pb\n",
      "/opt/ml/model/tensorflow/saved_model/0/variables,[],variables.index\n",
      "/opt/ml/model/tensorflow/saved_model/0/variables,[],variables.data-00000-of-00001\n",
      "/opt/ml/model/transformers/fine-tuned,[],tf_model.h5\n",
      "/opt/ml/model/transformers/fine-tuned,[],config.json\n",
      "/opt/ml/input/data/train,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/train,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "/opt/ml/input/data/test,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "/opt/ml/input/data/test,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/validation,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/validation,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "/opt/ml/code,[],train.py\n",
      "Done.\n",
      "Args:\n",
      "Namespace(checkpoint_base_path='/opt/ml/checkpoints', current_host='localhost', enable_checkpointing=False, enable_sagemaker_debugger=False, enable_tensorboard=False, epochs=1, epsilon=1e-08, freeze_bert_layer=True, hosts={'hosts': 'localhost'}, learning_rate=1e-05, max_seq_length=64, num_gpus=0, output_data_dir='/opt/ml/output/data/', output_dir='/opt/ml/output/', run_sample_predictions=False, run_test=False, run_validation=True, test_batch_size=18, test_data='/opt/ml/input/data/test', test_steps=1, train_batch_size=36, train_data='/opt/ml/input/data/train', train_steps_per_epoch=1, use_amp=False, use_xla=True, validation_batch_size=18, validation_data='/opt/ml/input/data/validation', validation_steps=1)\n",
      "Environment Variables:\n",
      "{'DEBCONF_NONINTERACTIVE_SEEN': 'true',\n",
      " 'DEBIAN_FRONTEND': 'noninteractive',\n",
      " 'HDF5_USE_FILE_LOCKING': 'FALSE',\n",
      " 'HOME': '/root',\n",
      " 'HOSTNAME': 'bert-model-training',\n",
      " 'KMP_AFFINITY': 'granularity=fine,compact,1,0',\n",
      " 'KMP_BLOCKTIME': '1',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'True',\n",
      " 'KMP_INIT_AT_FORK': 'FALSE',\n",
      " 'KMP_SETTINGS': '0',\n",
      " 'KUBERNETES_PORT': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP_ADDR': '10.100.0.1',\n",
      " 'KUBERNETES_PORT_443_TCP_PORT': '443',\n",
      " 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp',\n",
      " 'KUBERNETES_SERVICE_HOST': '10.100.0.1',\n",
      " 'KUBERNETES_SERVICE_PORT': '443',\n",
      " 'KUBERNETES_SERVICE_PORT_HTTPS': '443',\n",
      " 'LANG': 'C.UTF-8',\n",
      " 'LC_ALL': 'C.UTF-8',\n",
      " 'LD_LIBRARY_PATH': '/usr/local/openmpi/lib:',\n",
      " 'PATH': '/usr/local/openmpi/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
      " 'PYTHONDONTWRITEBYTECODE': '1',\n",
      " 'PYTHONIOENCODING': 'UTF-8',\n",
      " 'PYTHONUNBUFFERED': '1',\n",
      " 'SAGEMAKER_JOB_NAME': 'tf-bert-training-eks',\n",
      " 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_tensorflow_container.training:main',\n",
      " 'SM_CHANNEL_TEST': '/opt/ml/input/data/test',\n",
      " 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\n",
      " 'SM_CHANNEL_VALIDATION': '/opt/ml/input/data/validation',\n",
      " 'SM_CURRENT_HOST': 'localhost',\n",
      " 'SM_HOSTS': '{\"hosts\":\"localhost\"}',\n",
      " 'SM_MODEL_DIR': '/opt/ml/model/',\n",
      " 'SM_NUM_GPUS': '0',\n",
      " 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data/',\n",
      " 'SM_OUTPUT_DIR': '/opt/ml/output/',\n",
      " 'SM_TRAINING_ENV': '{\"is_master\":true}'}\n",
      "SM_TRAINING_ENV {\"is_master\":true}\n",
      "is_master True\n",
      "train_data /opt/ml/input/data/train\n",
      "validation_data /opt/ml/input/data/validation\n",
      "test_data /opt/ml/input/data/test\n",
      "output_dir /opt/ml/output/\n",
      "hosts {'hosts': 'localhost'}\n",
      "current_host localhost\n",
      "num_gpus 0\n",
      "job_name tf-bert-training-eks\n",
      "use_xla True\n",
      "use_amp False\n",
      "max_seq_length 64\n",
      "train_batch_size 36\n",
      "validation_batch_size 18\n",
      "test_batch_size 18\n",
      "epochs 1\n",
      "learning_rate 1e-05\n",
      "epsilon 1e-08\n",
      "train_steps_per_epoch 1\n",
      "validation_steps 1\n",
      "test_steps 1\n",
      "freeze_bert_layer True\n",
      "enable_sagemaker_debugger False\n",
      "run_validation True\n",
      "run_test False\n",
      "run_sample_predictions False\n",
      "enable_tensorboard False\n",
      "enable_checkpointing False\n",
      "checkpoint_base_path /opt/ml/checkpoints\n",
      "checkpoint_path /opt/ml/checkpoints\n",
      "Using pipe_mode: False\n",
      "2020-11-02 12:12:55.319547: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-11-02 12:12:55.343615: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\n",
      "2020-11-02 12:12:55.343871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5bf2fc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-02 12:12:55.343889: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-02 12:12:55.344035: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "train_data_filenames ['/opt/ml/input/data/train/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/train/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['/opt/ml/input/data/train/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/train/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "WARNING:tensorflow:From /opt/ml/code/train.py:88: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "**************** train *****************\n",
      "{'input_ids': array([[  101,  2079,  2025, ...,  2033,  2055,   102],\n",
      "       [  101, 12202,  2003, ...,  7595,  2006,   102],\n",
      "       [  101,  2562,  1999, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2561, 19380, ...,  2008,  2052,   102],\n",
      "       [  101,  2224,  2009, ...,     0,     0,     0],\n",
      "       [  101,  1045,  2507, ...,  1010,  2941,   102]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1]]), 'label_ids': array([0, 2, 4, 3, 3, 2, 4, 2, 4, 2, 4, 4, 3, 3, 4, 4, 2, 4, 3, 2, 0, 1,\n",
      "       0, 4, 3, 4, 1, 3, 3, 0, 2, 2, 4, 1, 4, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  2026,  1023, ...,     0,     0,     0],\n",
      "       [  101,  1045,  4149, ...,     0,     0,     0],\n",
      "       [  101,  2204,  4031, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1045, 18991, ...,     0,     0,     0],\n",
      "       [  101,  2092,  1011, ...,  2059,  3225,   102],\n",
      "       [  101,  2009,  1005, ...,     0,     0,     0]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([4, 2, 4, 3, 2, 0, 1, 0, 4, 3, 4, 1, 3, 3, 0, 2, 2, 4, 1, 4, 3, 3,\n",
      "       0, 4, 1, 0, 2, 4, 0, 4, 0, 2, 3, 0, 0, 2]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  2785,  1997, ...,  7484,  6681,   102],\n",
      "       [  101,  2499,  2092, ...,     0,     0,     0],\n",
      "       [  101,  1045, 18991, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1045,  1005, ...,   999,  1026,   102],\n",
      "       [  101, 12202,  2444, ...,  2769,  1012,   102],\n",
      "       [  101,  2307,  2399, ...,     0,     0,     0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 0, 0, 2, 0, 4, 1, 1, 3, 0, 1, 3, 4, 0, 3, 4, 4, 2, 4, 2, 0,\n",
      "       2, 4, 3, 3, 2, 4, 2, 4, 2, 4, 4, 3, 3, 4]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  1045, 18991, ...,     0,     0,     0],\n",
      "       [  101,  2092,  1011, ...,  2059,  3225,   102],\n",
      "       [  101,  2009,  1005, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2307,  2399, ...,     0,     0,     0],\n",
      "       [  101,  2026,  1023, ...,     0,     0,     0],\n",
      "       [  101,  1045,  4149, ...,     0,     0,     0]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([0, 0, 2, 0, 4, 1, 1, 3, 0, 1, 3, 4, 0, 3, 4, 4, 2, 4, 2, 0, 2, 4,\n",
      "       3, 3, 2, 4, 2, 4, 2, 4, 4, 3, 3, 4, 4, 2]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  1045,  1005, ...,  1998,  2005,   102],\n",
      "       [  101,  1045,  4149, ...,     0,     0,     0],\n",
      "       [  101,  2065,  2045, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 22817,  2023, ...,     0,     0,     0],\n",
      "       [  101,  2023,  2097, ...,  6097,  1012,   102],\n",
      "       [  101,  4007,  2573, ...,  2185,  2007,   102]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]]), 'label_ids': array([1, 3, 0, 1, 3, 4, 0, 3, 4, 4, 2, 4, 2, 0, 2, 4, 3, 3, 2, 4, 2, 4,\n",
      "       2, 4, 4, 3, 3, 4, 4, 2, 4, 3, 2, 0, 1, 0]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 2009, 2573, ...,    0,    0,    0],\n",
      "       [ 101, 4569, 2050, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2031, ..., 2412, 5278,  102],\n",
      "       ...,\n",
      "       [ 101, 1045, 4149, ..., 2003, 2204,  102],\n",
      "       [ 101, 2097, 2196, ..., 1996, 2208,  102],\n",
      "       [ 101, 2043, 1045, ..., 1999, 1996,  102]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]]), 'label_ids': array([4, 1, 0, 2, 4, 0, 4, 0, 2, 3, 0, 0, 2, 0, 4, 1, 1, 3, 0, 1, 3, 4,\n",
      "       0, 3, 4, 4, 2, 4, 2, 0, 2, 4, 3, 3, 2, 4]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.19MB/s]\n",
      "Downloading: 100%|██████████| 442/442 [00:00<00:00, 472kB/s]\n",
      "Downloading: 100%|██████████| 363M/363M [00:16<00:00, 21.5MB/s] \n",
      "2020-11-02 12:13:14.822939: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Sucessfully downloaded after 0 retries.\n",
      "** use_amp False\n",
      "enable_sagemaker_debugger False\n",
      "*** OPTIMIZER <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd7cc2dc3c8> ***\n",
      "Compiled model <transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x7fd7cc2dc2e8>\n",
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,957,317\n",
      "Trainable params: 594,437\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n",
      "None\n",
      "validation_data_filenames ['/opt/ml/input/data/validation/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/validation/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['/opt/ml/input/data/validation/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/validation/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "**************** validation *****************\n",
      "{'input_ids': array([[ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       ...,\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       ...,\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "{'input_ids': array([[ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "Starting Training and Validation...\n",
      "Train for 1 steps, validate for 1 steps\n",
      "2020-11-02 12:13:25.362585: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1574] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.6177 - accuracy: 0.2222 - val_loss: 1.5983 - val_accuracy: 0.2222\n",
      "2020-11-02 12:13:32.580039: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "<tensorflow.python.keras.callbacks.History object at 0x7fd77b740898>\n",
      "transformer_fine_tuned_model_path /opt/ml/model/transformers/fine-tuned/\n",
      "INFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/transformers/fine-tuned/config.json\n",
      "INFO:transformers.modeling_tf_utils:Model weights saved in /opt/ml/model/transformers/fine-tuned/tf_model.h5\n",
      "tensorflow_saved_model_path /opt/ml/model/tensorflow/saved_model/0\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e4415c18>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e4415c18>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7cc31c4e0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7cc31c4e0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7cc32aba8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7cc32aba8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e40e02b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e40e02b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e40f1978>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e40f1978>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e410d080>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fd7e410d080>, because it is not built.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /opt/ml/model/tensorflow/saved_model/0/assets\n",
      "INFO:tensorflow:Assets written to: /opt/ml/model/tensorflow/saved_model/0/assets\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs -f bert-model-training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
