{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/aim317\n"
     ]
    }
   ],
   "source": [
    "home_dir = os.getcwd()\n",
    "print(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export SM_MODEL_DIR=/home/ec2-user/SageMaker/aim317/opt/ml/model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export SM_MODEL_DIR=$(pwd)/opt/ml/model\n",
    "echo \"export SM_MODEL_DIR=${SM_MODEL_DIR}\" | tee -a ~/.bash_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export SM_MODEL_DIR=${home_dir}/opt/ml/model'\n",
    "echo \"export SM_MODEL_DIR=${SM_MODEL_DIR}\" | tee -a ~/.bash_profile\n",
    "\n",
    "!export SM_MODEL_DIR='${home_dir}/opt/ml/model'\n",
    "!echo $SM_MODEL_DIR\n",
    "!export SM_INPUT_DIR='opt/ml/input'\n",
    "# SM_INPUT_DATA_CONFIG\n",
    "\n",
    "!export SM_OUTPUT_DIR='opt/ml/output'\n",
    "!export SM_OUTPUT_DATA_DIR='opt/ml/output/data/'\n",
    "\n",
    "!export SM_CHANNELS='[\"TRAIN\",\"TEST\", \"VALIDATION\"]'\n",
    "!export SM_CHANNEL_TRAIN='opt/ml/input/data/train'\n",
    "!export SM_CHANNEL_VALIDATION='opt/ml/input/data/validation'\n",
    "!export SM_CHANNEL_TEST='opt/ml/input/data/test'\n",
    "\n",
    "!export SM_CURRENT_HOST='localhost'\n",
    "!export SM_HOSTS='{\\\"hosts\\\":\\\"localhost\\\"}'\n",
    "\n",
    "!export SM_NUM_GPUS=0\n",
    "!export SM_OUTPUT_DATA_DIR='opt/ml/output/data/localhost'\n",
    "\n",
    "!export SAGEMAKER_JOB_NAME='bert_model_training'\n",
    "!export SM_TRAINING_ENV='{\\\"is_master\\\":true}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n",
    "validation_data\n",
    "test_data\n",
    "output_dir\n",
    "local_model_dir\n",
    "hosts\n",
    "current_host\n",
    "num_gpus\n",
    "job_name\n",
    "checkpoint_base_path\n",
    "use_xla\n",
    "use_amp\n",
    "max_seq_length\n",
    "train_batch_size\n",
    "validation_batch_size\n",
    "test_batch_size\n",
    "epochs\n",
    "learning_rate\n",
    "epsilon\n",
    "train_steps_per_epoch\n",
    "validation_steps\n",
    "test_steps\n",
    "freeze_bert_layer\n",
    "enable_sagemaker_debugger\n",
    "run_validation\n",
    "run_test\n",
    "run_sample_predictions\n",
    "enable_tensorboard\n",
    "enable_checkpointing\n",
    "output_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pprint\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'matplotlib==3.2.1'])\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.configuration_distilbert import DistilBertConfig\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "CLASSES = [1, 2, 3, 4, 5]\n",
    "\n",
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def file_based_input_dataset_builder(channel,\n",
    "                                     input_filenames,\n",
    "                                     pipe_mode,\n",
    "                                     is_training,\n",
    "                                     drop_remainder,\n",
    "                                     batch_size,\n",
    "                                     epochs,\n",
    "                                     steps_per_epoch,\n",
    "                                     max_seq_length):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print('***** Using pipe_mode with channel {}'.format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "        dataset = PipeModeDataset(channel=channel,\n",
    "                                  record_format='TFRecord')\n",
    "    else:\n",
    "        print('***** Using input_filenames {}'.format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(epochs * steps_per_epoch * 100)\n",
    "#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        record = tf.io.parse_single_example(record, name_to_features)\n",
    "        # TODO:  wip/bert/bert_attention_head_view/train.py\n",
    "        # Convert input_ids into input_tokens with DistilBert vocabulary \n",
    "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\n",
    "        #    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\n",
    "        return record\n",
    "    \n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "          lambda record: _decode_record(record, name_to_features),\n",
    "          batch_size=batch_size,\n",
    "          drop_remainder=drop_remainder,\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "#    dataset.cache()\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=1000,\n",
    "                              reshuffle_each_iteration=True)\n",
    "\n",
    "    row_count = 0\n",
    "    print('**************** {} *****************'.format(channel))\n",
    "    for row in dataset.as_numpy_iterator():\n",
    "        print(row)\n",
    "        if row_count == 5:\n",
    "            break\n",
    "        row_count = row_count + 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_checkpoint_model(checkpoint_path):\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    glob_pattern = os.path.join(checkpoint_path, '*.h5')\n",
    "    print('glob pattern {}'.format(glob_pattern))\n",
    "\n",
    "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
    "    print('List of checkpoint files {}'.format(list_of_checkpoint_files))\n",
    "    \n",
    "    latest_checkpoint_file = max(list_of_checkpoint_files)\n",
    "    print('Latest checkpoint file {}'.format(latest_checkpoint_file))\n",
    "\n",
    "    initial_epoch_number_str = latest_checkpoint_file.rsplit('_', 1)[-1].split('.h5')[0]\n",
    "    initial_epoch_number = int(initial_epoch_number_str)\n",
    "\n",
    "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                                               latest_checkpoint_file,\n",
    "                                               config=config)\n",
    "\n",
    "    print('loaded_model {}'.format(loaded_model))\n",
    "    print('initial_epoch_number {}'.format(initial_epoch_number))\n",
    "    \n",
    "    return loaded_model, initial_epoch_number\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--test_data',\n",
    "                        type=str,\n",
    "                        default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--output_dir',\n",
    "                        type=str,\n",
    "                        default=os.environ['SM_OUTPUT_DIR'])\n",
    "    parser.add_argument('--local_model_dir', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--hosts', \n",
    "                        type=list, \n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current_host', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CURRENT_HOST'])    \n",
    "    parser.add_argument('--num_gpus', \n",
    "                        type=int, \n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--job_name', \n",
    "                        type=str, \n",
    "                        default=os.environ['SAGEMAKER_JOB_NAME'])\n",
    "    parser.add_argument('--checkpoint_base_path', \n",
    "                        type=str, \n",
    "                        default='/opt/ml/checkpoints')\n",
    "    parser.add_argument('--use_xla',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--use_amp',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--max_seq_length',\n",
    "                        type=int,\n",
    "                        default=64)\n",
    "    parser.add_argument('--train_batch_size',\n",
    "                        type=int,\n",
    "                        default=128)\n",
    "    parser.add_argument('--validation_batch_size',\n",
    "                        type=int,\n",
    "                        default=256)\n",
    "    parser.add_argument('--test_batch_size',\n",
    "                        type=int,\n",
    "                        default=256)\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=2)\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type=float,\n",
    "                        default=0.00003)\n",
    "    parser.add_argument('--epsilon',\n",
    "                        type=float,\n",
    "                        default=0.00000001)\n",
    "    parser.add_argument('--train_steps_per_epoch',\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument('--validation_steps',\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument('--test_steps',\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument('--freeze_bert_layer',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--enable_sagemaker_debugger',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--run_validation',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--run_test',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--run_sample_predictions',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--enable_tensorboard',\n",
    "                        type=eval,\n",
    "                        default=False)        \n",
    "    parser.add_argument('--enable_checkpointing',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--output_data_dir', # This is unused\n",
    "                        type=str,\n",
    "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "     \n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Args:\") \n",
    "    print(args)\n",
    "    \n",
    "    env_var = os.environ \n",
    "    print(\"Environment Variables:\") \n",
    "    pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "    print('SM_TRAINING_ENV {}'.format(env_var['SM_TRAINING_ENV']))\n",
    "    sm_training_env_json = json.loads(env_var['SM_TRAINING_ENV'])\n",
    "    is_master = sm_training_env_json['is_master']\n",
    "    print('is_master {}'.format(is_master))\n",
    "    \n",
    "    train_data = args.train_data\n",
    "    print('train_data {}'.format(train_data))\n",
    "    validation_data = args.validation_data\n",
    "    print('validation_data {}'.format(validation_data))\n",
    "    test_data = args.test_data\n",
    "    print('test_data {}'.format(test_data))    \n",
    "    local_model_dir = args.local_model_dir\n",
    "    print('local_model_dir {}'.format(local_model_dir))\n",
    "    output_dir = args.output_dir\n",
    "    print('output_dir {}'.format(output_dir))    \n",
    "    hosts = args.hosts\n",
    "    print('hosts {}'.format(hosts))    \n",
    "    current_host = args.current_host\n",
    "    print('current_host {}'.format(current_host))    \n",
    "    num_gpus = args.num_gpus\n",
    "    print('num_gpus {}'.format(num_gpus))\n",
    "    job_name = args.job_name\n",
    "    print('job_name {}'.format(job_name))    \n",
    "    use_xla = args.use_xla\n",
    "    print('use_xla {}'.format(use_xla))    \n",
    "    use_amp = args.use_amp\n",
    "    print('use_amp {}'.format(use_amp))    \n",
    "    max_seq_length = args.max_seq_length\n",
    "    print('max_seq_length {}'.format(max_seq_length))    \n",
    "    train_batch_size = args.train_batch_size\n",
    "    print('train_batch_size {}'.format(train_batch_size))    \n",
    "    validation_batch_size = args.validation_batch_size\n",
    "    print('validation_batch_size {}'.format(validation_batch_size))    \n",
    "    test_batch_size = args.test_batch_size\n",
    "    print('test_batch_size {}'.format(test_batch_size))    \n",
    "    epochs = args.epochs\n",
    "    print('epochs {}'.format(epochs))    \n",
    "    learning_rate = args.learning_rate\n",
    "    print('learning_rate {}'.format(learning_rate))    \n",
    "    epsilon = args.epsilon\n",
    "    print('epsilon {}'.format(epsilon))    \n",
    "    train_steps_per_epoch = args.train_steps_per_epoch\n",
    "    print('train_steps_per_epoch {}'.format(train_steps_per_epoch))    \n",
    "    validation_steps = args.validation_steps\n",
    "    print('validation_steps {}'.format(validation_steps))    \n",
    "    test_steps = args.test_steps\n",
    "    print('test_steps {}'.format(test_steps))    \n",
    "    freeze_bert_layer = args.freeze_bert_layer\n",
    "    print('freeze_bert_layer {}'.format(freeze_bert_layer))    \n",
    "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\n",
    "    print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))    \n",
    "    run_validation = args.run_validation\n",
    "    print('run_validation {}'.format(run_validation))    \n",
    "    run_test = args.run_test\n",
    "    print('run_test {}'.format(run_test))    \n",
    "    run_sample_predictions = args.run_sample_predictions\n",
    "    print('run_sample_predictions {}'.format(run_sample_predictions))\n",
    "    enable_tensorboard = args.enable_tensorboard\n",
    "    print('enable_tensorboard {}'.format(enable_tensorboard))       \n",
    "    enable_checkpointing = args.enable_checkpointing\n",
    "    print('enable_checkpointing {}'.format(enable_checkpointing))    \n",
    "\n",
    "    checkpoint_base_path = args.checkpoint_base_path\n",
    "    print('checkpoint_base_path {}'.format(checkpoint_base_path))\n",
    "\n",
    "    if is_master:\n",
    "        checkpoint_path = checkpoint_base_path\n",
    "    else:\n",
    "        checkpoint_path = '/tmp/checkpoints'        \n",
    "    print('checkpoint_path {}'.format(checkpoint_path))\n",
    "    \n",
    "    # Determine if PipeMode is enabled \n",
    "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\n",
    "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\n",
    "    print('Using pipe_mode: {}'.format(pipe_mode))\n",
    " \n",
    "    # Model Output \n",
    "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, 'transformers/fine-tuned/')\n",
    "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=True)\n",
    "\n",
    "    # SavedModel Output\n",
    "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\n",
    "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n",
    "\n",
    "    # Tensorboard Logs \n",
    "    tensorboard_logs_path = os.path.join(local_model_dir, 'tensorboard/')\n",
    "    os.makedirs(tensorboard_logs_path, exist_ok=True)\n",
    "\n",
    "    # Commented out due to incompatibility with transformers library (possibly)\n",
    "    # Set the global precision mixed_precision policy to \"mixed_float16\"    \n",
    "#    mixed_precision_policy = 'mixed_float16'\n",
    "#    print('Mixed precision policy {}'.format(mixed_precision_policy))\n",
    "#    policy = mixed_precision.Policy(mixed_precision_policy)\n",
    "#    mixed_precision.set_policy(policy)    \n",
    "    \n",
    "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
    "    # Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\n",
    "    #distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    with distributed_strategy.scope():\n",
    "        tf.config.optimizer.set_jit(use_xla)\n",
    "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\n",
    "\n",
    "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\n",
    "        print('train_data_filenames {}'.format(train_data_filenames))\n",
    "        train_dataset = file_based_input_dataset_builder(\n",
    "            channel='train',\n",
    "            input_filenames=train_data_filenames,\n",
    "            pipe_mode=pipe_mode,\n",
    "            is_training=True,\n",
    "            drop_remainder=False,\n",
    "            batch_size=train_batch_size,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=train_steps_per_epoch,\n",
    "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "\n",
    "        tokenizer = None\n",
    "        config = None\n",
    "        model = None\n",
    "\n",
    "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\n",
    "        successful_download = False\n",
    "        retries = 0\n",
    "        while (retries < 5 and not successful_download):\n",
    "            try:\n",
    "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\n",
    "                                                          num_labels=len(CLASSES))\n",
    "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                                              config=config)\n",
    "                successful_download = True\n",
    "                print('Sucessfully downloaded after {} retries.'.format(retries))\n",
    "            except:\n",
    "                retries = retries + 1\n",
    "                random_sleep = random.randint(1, 30)\n",
    "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\n",
    "                time.sleep(random_sleep)\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        initial_epoch_number = 0 \n",
    "\n",
    "        if enable_checkpointing:\n",
    "            print('***** Checkpoint enabled *****')\n",
    "            \n",
    "            os.makedirs(checkpoint_path, exist_ok=True)        \n",
    "            if os.listdir(checkpoint_path):\n",
    "                print('***** Found checkpoint *****')\n",
    "                print(checkpoint_path)\n",
    "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
    "                print('***** Using checkpoint model {} *****'.format(model))\n",
    "                \n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                    filepath=os.path.join(checkpoint_path, 'tf_model_{epoch:05d}.h5'),\n",
    "                    save_weights_only=False,\n",
    "                    verbose=1,\n",
    "                    monitor='val_accuracy')\n",
    "            print('*** CHECKPOINT CALLBACK {} ***'.format(checkpoint_callback))\n",
    "            callbacks.append(checkpoint_callback)\n",
    "\n",
    "        if not tokenizer or not model or not config:\n",
    "            print('Not properly initialized...')\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "        print('** use_amp {}'.format(use_amp))        \n",
    "        if use_amp:\n",
    "            # loss scaling is currently required when using mixed precision\n",
    "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\n",
    "\n",
    "        print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))\n",
    "        if enable_sagemaker_debugger:\n",
    "            print('*** DEBUGGING ***')\n",
    "            import smdebug.tensorflow as smd\n",
    "            # This assumes that we specified debugger_hook_config\n",
    "            debugger_callback = smd.KerasHook.create_from_json_file()\n",
    "            print('*** DEBUGGER CALLBACK {} ***'.format(debugger_callback))            \n",
    "            callbacks.append(debugger_callback)\n",
    "            optimizer = debugger_callback.wrap_optimizer(optimizer)\n",
    "\n",
    "        if enable_tensorboard:            \n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "                                                        log_dir=tensorboard_logs_path)\n",
    "            print('*** TENSORBOARD CALLBACK {} ***'.format(tensorboard_callback))\n",
    "            callbacks.append(tensorboard_callback)\n",
    "  \n",
    "        print('*** OPTIMIZER {} ***'.format(optimizer))\n",
    "        \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "        print('Compiled model {}'.format(model))          \n",
    "        model.layers[0].trainable = not freeze_bert_layer\n",
    "        print(model.summary())\n",
    "\n",
    "        if run_validation:\n",
    "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\n",
    "            print('validation_data_filenames {}'.format(validation_data_filenames))\n",
    "            validation_dataset = file_based_input_dataset_builder(\n",
    "                channel='validation',\n",
    "                input_filenames=validation_data_filenames,\n",
    "                pipe_mode=pipe_mode,\n",
    "                is_training=False,\n",
    "                drop_remainder=False,\n",
    "                batch_size=validation_batch_size,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=validation_steps,\n",
    "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "            \n",
    "            print('Starting Training and Validation...')\n",
    "            validation_dataset = validation_dataset.take(validation_steps)\n",
    "            train_and_validation_history = model.fit(train_dataset,\n",
    "                                                     shuffle=True,\n",
    "                                                     epochs=epochs,\n",
    "                                                     initial_epoch=initial_epoch_number,\n",
    "                                                     steps_per_epoch=train_steps_per_epoch,\n",
    "                                                     validation_data=validation_dataset,\n",
    "                                                     validation_steps=validation_steps,\n",
    "                                                     callbacks=callbacks)                                \n",
    "            print(train_and_validation_history)\n",
    "        else: # Not running validation\n",
    "            print('Starting Training (Without Validation)...')\n",
    "            train_history = model.fit(train_dataset,\n",
    "                                      shuffle=True,\n",
    "                                      epochs=epochs,\n",
    "                                      initial_epoch=initial_epoch_number,\n",
    "                                      steps_per_epoch=train_steps_per_epoch,\n",
    "                                      callbacks=callbacks)                \n",
    "            print(train_history)\n",
    "\n",
    "        if run_test:\n",
    "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\n",
    "            print('test_data_filenames {}'.format(test_data_filenames))\n",
    "            test_dataset = file_based_input_dataset_builder(\n",
    "                channel='test',\n",
    "                input_filenames=test_data_filenames,\n",
    "                pipe_mode=pipe_mode,\n",
    "                is_training=False,\n",
    "                drop_remainder=False,\n",
    "                batch_size=test_batch_size,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=test_steps,\n",
    "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "\n",
    "            print('Starting test...')\n",
    "            test_history = model.evaluate(test_dataset,\n",
    "                                          steps=test_steps,\n",
    "                                          callbacks=callbacks)\n",
    "                                 \n",
    "            print('Test history {}'.format(test_history))\n",
    "            \n",
    "        # Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\n",
    "        print('transformer_fine_tuned_model_path {}'.format(transformer_fine_tuned_model_path))   \n",
    "        model.save_pretrained(transformer_fine_tuned_model_path)\n",
    "\n",
    "        # Save the TensorFlow SavedModel for Serving Predictions\n",
    "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \n",
    "        model.save(tensorflow_saved_model_path, save_format='tf')\n",
    "                \n",
    "        # Copy inference.py and requirements.txt to the code/ directory\n",
    "        #   Note: This is required for the SageMaker Endpoint to pick them up.\n",
    "        #         This appears to be hard-coded and must be called code/\n",
    "        inference_path = os.path.join(local_model_dir, 'code/')\n",
    "        print('Copying inference source files to {}'.format(inference_path))\n",
    "        os.makedirs(inference_path, exist_ok=True)               \n",
    "        os.system('cp inference.py {}'.format(inference_path))\n",
    "        print(glob(inference_path))        \n",
    "#        os.system('cp requirements.txt {}/code'.format(inference_path))\n",
    "        \n",
    "    if run_sample_predictions:\n",
    "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\n",
    "                                                                       id2label={\n",
    "                                                                        0: 1,\n",
    "                                                                        1: 2,\n",
    "                                                                        2: 3,\n",
    "                                                                        3: 4,\n",
    "                                                                        4: 5\n",
    "                                                                       },\n",
    "                                                                       label2id={\n",
    "                                                                        1: 0,\n",
    "                                                                        2: 1,\n",
    "                                                                        3: 2,\n",
    "                                                                        4: 3,\n",
    "                                                                        5: 4\n",
    "                                                                       })\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        if num_gpus >= 1:\n",
    "            inference_device = 0 # GPU 0\n",
    "        else:\n",
    "            inference_device = -1 # CPU\n",
    "        print('inference_device {}'.format(inference_device))\n",
    "\n",
    "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        framework='tf',\n",
    "                                                        device=inference_device)  \n",
    "\n",
    "        print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\n",
    "        print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\n",
    "        print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\n",
    "\n",
    "        import csv\n",
    "\n",
    "        df_test_reviews = pd.read_csv('./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \n",
    "                                        delimiter='\\t', \n",
    "                                        quoting=csv.QUOTE_NONE,\n",
    "                                        compression='gzip')[['review_body', 'star_rating']]\n",
    "\n",
    "        df_test_reviews = df_test_reviews.sample(n=100)\n",
    "        df_test_reviews.shape\n",
    "        df_test_reviews.head()\n",
    "        \n",
    "        import pandas as pd\n",
    "\n",
    "        def predict(review_body):\n",
    "            prediction_map = inference_pipeline(review_body)\n",
    "            return prediction_map[0]['label']\n",
    "\n",
    "        y_test = df_test_reviews['review_body'].map(predict)\n",
    "        y_test\n",
    "        \n",
    "        y_actual = df_test_reviews['star_rating']\n",
    "        y_actual\n",
    "\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(y_true=y_test, y_pred=y_actual))\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        print('Accuracy: ', accuracy_score(y_true=y_test, y_pred=y_actual))\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        import pandas as pd\n",
    "\n",
    "        def plot_conf_mat(cm, classes, title, cmap = plt.cm.Greens):\n",
    "            print(cm)\n",
    "            plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "            plt.title(title)\n",
    "            plt.colorbar()\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "\n",
    "            fmt = 'd'\n",
    "            thresh = cm.max() / 2.\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.ylabel('True label')\n",
    "                plt.xlabel('Predicted label')\n",
    "                \n",
    "        import itertools\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import matplotlib.pyplot as plt\n",
    "        #%matplotlib inline\n",
    "        #%config InlineBackend.figure_format='retina'\n",
    "\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
    "\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        plot_conf_mat(cm, \n",
    "                      classes=['1', '2', '3', '4', '5'], \n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "        # Save the confusion matrix        \n",
    "        plt.show()\n",
    "        \n",
    "        # Model Output \n",
    "        metrics_path = os.path.join(local_model_dir, 'metrics/')\n",
    "        os.makedirs(metrics_path, exist_ok=True)\n",
    "        plt.savefig('{}/confusion_matrix.png'.format(metrics_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python opt/ml/code/dev-train.py \\\n",
    "    --train_steps_per_epoch=100 \\\n",
    "    --epochs=10 \\\n",
    "    --learning_rate=0.00001 \\\n",
    "    --epsilon=0.00000001 \\\n",
    "    --train_batch_size=128 \\\n",
    "    --validation_batch_size=64 \\\n",
    "    --test_batch_size=64 \\\n",
    "    --validation_steps=10 \\\n",
    "    --test_steps=10 \\\n",
    "    --use_xla=True \\\n",
    "    --use_amp=False \\\n",
    "    --max_seq_length=64 \\\n",
    "    --freeze_bert_layer=True \\\n",
    "    --enable_sagemaker_debugger=True \\\n",
    "    --enable_checkpointing=True \\\n",
    "    --enable_tensorboard=True \\\n",
    "    --run_validation=True \\\n",
    "    --run_test=True \\\n",
    "    --run_sample_predictions=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
