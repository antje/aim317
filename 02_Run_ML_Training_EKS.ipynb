{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a BERT Model and Create a Text Classifier\n",
    "\n",
    "We have already performed the Feature Engineering to create BERT embeddings from the `reviews_body` text using the pre-trained BERT model, and split the dataset into train, validation and test files. To optimize for Tensorflow training, we saved the files in TFRecord format. \n",
    "\n",
    "Now, let’s fine-tune the BERT model to our Customer Reviews Dataset and add a new classification layer to predict the `star_rating` for a given `review_body`.\n",
    "\n",
    "![BERT Training](img/bert_training.png)\n",
    "\n",
    "As mentioned earlier, BERT’s attention mechanism is called a Transformer. This is, not coincidentally, the name of the popular BERT Python library, “Transformers,” maintained by a company called [HuggingFace](https://github.com/huggingface/transformers). We will use a variant of BERT called [DistilBert](https://arxiv.org/pdf/1910.01108.pdf) which requires less memory and compute, but maintains very good accuracy on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 2: \n",
    "\n",
    "# Run Model Training on Amazon Elastic Kubernetes Service (Amazon EKS)\n",
    "\n",
    "Amazon EKS is a managed service that makes it easy for you to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon FSx For Lustre\n",
    "\n",
    "Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage.\n",
    "\n",
    "Powered by Lustre, the world's most popular high-performance file system, FSx for Lustre offers sub-millisecond latencies, up to hundreds of gigabytes per second of throughput, and millions of IOPS. It provides multiple deployment options and storage types to optimize cost and performance for your workload requirements.\n",
    "\n",
    "FSx for Lustre file systems can also be linked to Amazon S3 buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon FSx for Lustre Container Storage Interface (CSI) \n",
    "\n",
    "The Amazon FSx for Lustre Container Storage Interface (CSI)  driver provides a CSI interface that allows Amazon EKS clusters to manage the lifecycle of Amazon FSx for Lustre file systems. \n",
    "\n",
    "* https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html\n",
    "* https://github.com/kubernetes-sigs/aws-fsx-csi-driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "code/\n",
    "\ttrain.py\n",
    "\n",
    "input/\n",
    "\tdata/\n",
    "\t\ttest/\n",
    "\t\t\t*.tfrecord\n",
    "\t\ttrain/\n",
    "\t\t\t*.tfrecord\n",
    "\t\tvalidation/\n",
    "\t\t\t*.tfrecord\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List FSx Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-22 17:20:35          0 code/\n",
      "2020-11-22 17:20:54      17519 code/train.py\n",
      "2020-11-22 17:21:24        615 input/data/test/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-11-22 17:21:24        632 input/data/test/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-11-22 17:21:24      10728 input/data/train/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-11-22 17:21:24      11812 input/data/train/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-11-22 17:21:24        679 input/data/validation/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-11-22 17:21:24        642 input/data/validation/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive s3://fsx-container-demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Code `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.1.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TextClassificationPipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mconfiguration_distilbert\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\n",
      "    x = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    }\n",
      "\n",
      "    y = record[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(channel,\n",
      "                                     input_filenames,\n",
      "                                     pipe_mode,\n",
      "                                     is_training,\n",
      "                                     drop_remainder,\n",
      "                                     batch_size,\n",
      "                                     epochs,\n",
      "                                     steps_per_epoch,\n",
      "                                     max_seq_length):\n",
      "\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\n",
      "        dataset = PipeModeDataset(channel=channel,\n",
      "                                  record_format=\u001b[33m'\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_filenames))\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
      "\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\n",
      "\n",
      "    name_to_features = {\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\n",
      "    \n",
      "    dataset = dataset.apply(\n",
      "        tf.data.experimental.map_and_batch(\n",
      "          \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\n",
      "          batch_size=batch_size,\n",
      "          drop_remainder=drop_remainder,\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
      "\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m,\n",
      "                              reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m1\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing /opt...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mfor\u001b[39;49;00m root, subFolder, files \u001b[35min\u001b[39;49;00m os.walk(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "        \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m files:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(root, subFolder, item))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDone.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])  \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_data_config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--local_model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m100\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)         \n",
      "     \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    validation_data = args.validation_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\n",
      "    test_data = args.test_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))    \n",
      "    local_model_dir = args.local_model_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlocal_model_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))            \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))   \n",
      "    use_xla = args.use_xla\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_xla))    \n",
      "    use_amp = args.use_amp\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))    \n",
      "    max_seq_length = args.max_seq_length\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))    \n",
      "    train_batch_size = args.train_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_batch_size))    \n",
      "    validation_batch_size = args.validation_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_batch_size))    \n",
      "    test_batch_size = args.test_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_batch_size))    \n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))    \n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))    \n",
      "    epsilon = args.epsilon\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epsilon))    \n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_steps_per_epoch))    \n",
      "    validation_steps = args.validation_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_steps))    \n",
      "    test_steps = args.test_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_steps))    \n",
      "    freeze_bert_layer = args.freeze_bert_layer\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(freeze_bert_layer))       \n",
      "    run_validation = args.run_validation\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_validation))    \n",
      "    run_test = args.run_test\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_test))    \n",
      "    run_sample_predictions = args.run_sample_predictions\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_sample_predictions)) \n",
      "    input_data_config = args.input_data_config\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minput_data_config \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_data_config))\n",
      "\n",
      "    \n",
      "    \u001b[37m# Determine if PipeMode is enabled \u001b[39;49;00m\n",
      "    pipe_mode = (input_data_config.find(\u001b[33m'\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(pipe_mode))\n",
      " \n",
      "    \u001b[37m# Model Output \u001b[39;49;00m\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m) \n",
      "    \n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\n",
      "        tf.config.optimizer.set_jit(use_xla)\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\n",
      "\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data_filenames))\n",
      "        train_dataset = file_based_input_dataset_builder(\n",
      "            channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            input_filenames=train_data_filenames,\n",
      "            pipe_mode=pipe_mode,\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            batch_size=train_batch_size,\n",
      "            epochs=epochs,\n",
      "            steps_per_epoch=train_steps_per_epoch,\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m (retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download):\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES))\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                                              config=config)\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries))\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries, random_sleep))\n",
      "                time.sleep(random_sleep)\n",
      "\n",
      "        callbacks = []\n",
      "\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m \n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))        \n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m'\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "  \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "        \n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))          \n",
      "        model.layers[\u001b[34m0\u001b[39;49;00m].trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data_filenames))\n",
      "            validation_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=validation_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=validation_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=validation_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "            \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\n",
      "            train_and_validation_history = model.fit(train_dataset,\n",
      "                                                     shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                     epochs=epochs,\n",
      "                                                     initial_epoch=initial_epoch_number,\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\n",
      "                                                     validation_data=validation_dataset,\n",
      "                                                     validation_steps=validation_steps,\n",
      "                                                     callbacks=callbacks)                                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\n",
      "        \u001b[34melse\u001b[39;49;00m: \u001b[37m# Not running validation\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            train_history = model.fit(train_dataset,\n",
      "                                      shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                      epochs=epochs,\n",
      "                                      initial_epoch=initial_epoch_number,\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\n",
      "                                      callbacks=callbacks)                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_filenames))\n",
      "            test_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=test_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=test_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=test_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            test_history = model.evaluate(test_dataset,\n",
      "                                          steps=test_steps,\n",
      "                                          callbacks=callbacks)\n",
      "                                 \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_history))\n",
      "            \n",
      "        \u001b[37m# Save the Fine-Tuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformer_fine_tuned_model_path))   \n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\n",
      "\n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))   \n",
      "        model.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)       \n",
      "        \n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\n",
      "                                                                       id2label={\n",
      "                                                                        \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\n",
      "                                                                       },\n",
      "                                                                       label2id={\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\n",
      "                                                                       })\n",
      "\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m num_gpus >= \u001b[34m1\u001b[39;49;00m:\n",
      "            inference_device = \u001b[34m0\u001b[39;49;00m \u001b[37m# GPU 0\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            inference_device = -\u001b[34m1\u001b[39;49;00m \u001b[37m# CPU\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minference_device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_device))\n",
      "\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
      "                                                        tokenizer=tokenizer,\n",
      "                                                        framework=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                        device=inference_device)  \n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `train.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m \n",
      "\u001b[94mapiVersion\u001b[39;49;00m: v1\n",
      "\u001b[94mkind\u001b[39;49;00m: Pod\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: bert-model-training\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mvolumes\u001b[39;49;00m:\n",
      "  - \u001b[94mname\u001b[39;49;00m: fsx-opt-ml\n",
      "    \u001b[94mpersistentVolumeClaim\u001b[39;49;00m:\n",
      "      \u001b[94mclaimName\u001b[39;49;00m: fsx-claim\n",
      "  \u001b[94mcontainers\u001b[39;49;00m: \n",
      "    - \u001b[94mname\u001b[39;49;00m: bert\n",
      "      \u001b[94mcommand\u001b[39;49;00m: \n",
      "        - python\n",
      "        - /opt/ml/code/train.py\n",
      "        - --epochs=3\n",
      "        - --learning_rate=0.00001\n",
      "        - --epsilon=0.00000001\n",
      "        - --train_batch_size=128\n",
      "        - --validation_batch_size=64\n",
      "        - --test_batch_size=64\n",
      "        - --train_steps_per_epoch=100\n",
      "        - --validation_steps=10\n",
      "        - --test_steps=10\n",
      "        - --use_xla=True\n",
      "        - --use_amp=False\n",
      "        - --max_seq_length=64\n",
      "        - --freeze_bert_layer=True\n",
      "        - --run_validation=True\n",
      "        - --run_test=True\n",
      "        - --run_sample_predictions=True\n",
      "      \u001b[94mimage\u001b[39;49;00m: 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\n",
      "      \u001b[94mimagePullPolicy\u001b[39;49;00m: Always\n",
      "      \u001b[94msecurityContext\u001b[39;49;00m:\n",
      "        \u001b[94mprivileged\u001b[39;49;00m: true\n",
      "      \u001b[94mvolumeMounts\u001b[39;49;00m:\n",
      "      - \u001b[94mmountPath\u001b[39;49;00m: /opt/ml/\n",
      "        \u001b[94mname\u001b[39;49;00m: fsx-opt-ml\n",
      "      \u001b[94menv\u001b[39;49;00m: \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_NUM_GPUS\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_INPUT_DATA_CONFIG\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mFile\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_MODEL_DIR\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m     \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_OUTPUT_DIR\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_TRAIN\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_VALIDATION\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m     \n",
      "        - \u001b[94mname\u001b[39;49;00m: SM_CHANNEL_TEST\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        - \u001b[94mname\u001b[39;49;00m: HDF5_USE_FILE_LOCKING\n",
      "          \u001b[94mvalue\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mFALSE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "  \u001b[94mrestartPolicy\u001b[39;49;00m: Never\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Kubernetes Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                           STATUS   ROLES    AGE   VERSION\n",
      "ip-192-168-12-220.us-west-2.compute.internal   Ready    <none>   58m   v1.18.9-eks-d1db3c\n",
      "ip-192-168-27-75.us-west-2.compute.internal    Ready    <none>   58m   v1.18.9-eks-d1db3c\n",
      "ip-192-168-32-76.us-west-2.compute.internal    Ready    <none>   58m   v1.18.9-eks-d1db3c\n",
      "ip-192-168-54-75.us-west-2.compute.internal    Ready    <none>   58m   v1.18.9-eks-d1db3c\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kubectl delete -f train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/bert-model-training created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  READY   STATUS    RESTARTS   AGE\n",
      "bert-model-training   1/1     Running   0          72s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  READY   STATUS    RESTARTS   AGE\n",
      "bert-model-training   1/1     Running   0          75s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod bert-model-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         bert-model-training\n",
      "Namespace:    default\n",
      "Priority:     0\n",
      "Node:         ip-192-168-54-75.us-west-2.compute.internal/192.168.54.75\n",
      "Start Time:   Sun, 22 Nov 2020 17:48:20 +0000\n",
      "Labels:       <none>\n",
      "Annotations:  kubernetes.io/psp: eks.privileged\n",
      "Status:       Running\n",
      "IP:           192.168.58.13\n",
      "IPs:\n",
      "  IP:  192.168.58.13\n",
      "Containers:\n",
      "  bert:\n",
      "    Container ID:  docker://a3f1f9c6f14b84f7023ea07d2f1ff167f1eff8cbcead11b3a10254ab98cdba3a\n",
      "    Image:         763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\n",
      "    Image ID:      docker-pullable://763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training@sha256:4911ac31a130c68a2f92b72dd81d22bd02b542cc549c5652f22c1f24e702eaf5\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      python\n",
      "      /opt/ml/code/train.py\n",
      "      --epochs=3\n",
      "      --learning_rate=0.00001\n",
      "      --epsilon=0.00000001\n",
      "      --train_batch_size=128\n",
      "      --validation_batch_size=64\n",
      "      --test_batch_size=64\n",
      "      --train_steps_per_epoch=100\n",
      "      --validation_steps=10\n",
      "      --test_steps=10\n",
      "      --use_xla=True\n",
      "      --use_amp=False\n",
      "      --max_seq_length=64\n",
      "      --freeze_bert_layer=True\n",
      "      --run_validation=True\n",
      "      --run_test=True\n",
      "      --run_sample_predictions=True\n",
      "    State:          Running\n",
      "      Started:      Sun, 22 Nov 2020 17:48:54 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      SM_NUM_GPUS:            0\n",
      "      SM_INPUT_DATA_CONFIG:   File\n",
      "      SM_MODEL_DIR:           /opt/ml/model/\n",
      "      SM_OUTPUT_DIR:          /opt/ml/output/\n",
      "      SM_CHANNEL_TRAIN:       /opt/ml/input/data/train\n",
      "      SM_CHANNEL_VALIDATION:  /opt/ml/input/data/validation\n",
      "      SM_CHANNEL_TEST:        /opt/ml/input/data/test\n",
      "      HDF5_USE_FILE_LOCKING:  FALSE\n",
      "    Mounts:\n",
      "      /opt/ml/ from fsx-opt-ml (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jbnw6 (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  fsx-opt-ml:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  fsx-claim\n",
      "    ReadOnly:   false\n",
      "  default-token-jbnw6:\n",
      "    Type:        Secret (a volume populated by a Secret)\n",
      "    SecretName:  default-token-jbnw6\n",
      "    Optional:    false\n",
      "QoS Class:       BestEffort\n",
      "Node-Selectors:  <none>\n",
      "Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n",
      "                 node.kubernetes.io/unreachable:NoExecute for 300s\n",
      "Events:\n",
      "  Type    Reason     Age   From                                                  Message\n",
      "  ----    ------     ----  ----                                                  -------\n",
      "  Normal  Scheduled  79s   default-scheduler                                     Successfully assigned default/bert-model-training to ip-192-168-54-75.us-west-2.compute.internal\n",
      "  Normal  Pulling    76s   kubelet, ip-192-168-54-75.us-west-2.compute.internal  Pulling image \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\"\n",
      "  Normal  Pulled     51s   kubelet, ip-192-168-54-75.us-west-2.compute.internal  Successfully pulled image \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py36-ubuntu18.04\"\n",
      "  Normal  Created    45s   kubelet, ip-192-168-54-75.us-west-2.compute.internal  Created container bert\n",
      "  Normal  Started    45s   kubelet, ip-192-168-54-75.us-west-2.compute.internal  Started container bert\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod bert-model-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Training Job Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.22.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.53.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.43)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers==2.8.0) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=cb7059127464bcdaec2a8a8f0601804792e537342ab85dd41ef6e21b3aae0aa8\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, filelock, tqdm, dataclasses, sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed dataclasses-0.8 filelock-3.0.12 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 tqdm-4.53.0 transformers-2.8.0\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Requirement already satisfied: sagemaker-tensorflow==2.1.0.1.0.0 in /usr/local/lib/python3.6/dist-packages (2.1.0.1.0.0)\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Collecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22\n",
      "    Uninstalling scikit-learn-0.22:\n",
      "      Successfully uninstalled scikit-learn-0.22\n",
      "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "Environment Variables:\n",
      "{'DEBCONF_NONINTERACTIVE_SEEN': 'true',\n",
      " 'DEBIAN_FRONTEND': 'noninteractive',\n",
      " 'HDF5_USE_FILE_LOCKING': 'FALSE',\n",
      " 'HOME': '/root',\n",
      " 'HOSTNAME': 'bert-model-training',\n",
      " 'KMP_AFFINITY': 'granularity=fine,compact,1,0',\n",
      " 'KMP_BLOCKTIME': '1',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'True',\n",
      " 'KMP_INIT_AT_FORK': 'FALSE',\n",
      " 'KMP_SETTINGS': '0',\n",
      " 'KUBERNETES_PORT': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP': 'tcp://10.100.0.1:443',\n",
      " 'KUBERNETES_PORT_443_TCP_ADDR': '10.100.0.1',\n",
      " 'KUBERNETES_PORT_443_TCP_PORT': '443',\n",
      " 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp',\n",
      " 'KUBERNETES_SERVICE_HOST': '10.100.0.1',\n",
      " 'KUBERNETES_SERVICE_PORT': '443',\n",
      " 'KUBERNETES_SERVICE_PORT_HTTPS': '443',\n",
      " 'LANG': 'C.UTF-8',\n",
      " 'LC_ALL': 'C.UTF-8',\n",
      " 'LD_LIBRARY_PATH': '/usr/local/openmpi/lib:',\n",
      " 'PATH': '/usr/local/openmpi/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
      " 'PYTHONDONTWRITEBYTECODE': '1',\n",
      " 'PYTHONIOENCODING': 'UTF-8',\n",
      " 'PYTHONUNBUFFERED': '1',\n",
      " 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_tensorflow_container.training:main',\n",
      " 'SM_CHANNEL_TEST': '/opt/ml/input/data/test',\n",
      " 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train',\n",
      " 'SM_CHANNEL_VALIDATION': '/opt/ml/input/data/validation',\n",
      " 'SM_INPUT_DATA_CONFIG': 'File',\n",
      " 'SM_MODEL_DIR': '/opt/ml/model/',\n",
      " 'SM_NUM_GPUS': '0',\n",
      " 'SM_OUTPUT_DIR': '/opt/ml/output/'}\n",
      "Listing /opt...\n",
      "/opt/ml/code,[],train.py\n",
      "/opt/ml/input/data/validation,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/validation,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "/opt/ml/input/data/test,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "/opt/ml/input/data/test,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/train,[],part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "/opt/ml/input/data/train,[],part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "Done.\n",
      "Args:\n",
      "Namespace(epochs=3, epsilon=1e-08, freeze_bert_layer=True, input_data_config='File', learning_rate=1e-05, local_model_dir='/opt/ml/model/', max_seq_length=64, num_gpus=0, run_sample_predictions=True, run_test=True, run_validation=True, test_batch_size=64, test_data='/opt/ml/input/data/test', test_steps=10, train_batch_size=128, train_data='/opt/ml/input/data/train', train_steps_per_epoch=100, use_amp=False, use_xla=True, validation_batch_size=64, validation_data='/opt/ml/input/data/validation', validation_steps=10)\n",
      "train_data /opt/ml/input/data/train\n",
      "validation_data /opt/ml/input/data/validation\n",
      "test_data /opt/ml/input/data/test\n",
      "local_model_dir /opt/ml/model/\n",
      "num_gpus 0\n",
      "use_xla True\n",
      "use_amp False\n",
      "max_seq_length 64\n",
      "train_batch_size 128\n",
      "validation_batch_size 64\n",
      "test_batch_size 64\n",
      "epochs 3\n",
      "learning_rate 1e-05\n",
      "epsilon 1e-08\n",
      "train_steps_per_epoch 100\n",
      "validation_steps 10\n",
      "test_steps 10\n",
      "freeze_bert_layer True\n",
      "run_validation True\n",
      "run_test True\n",
      "run_sample_predictions True\n",
      "input_data_config File\n",
      "Using pipe_mode: False\n",
      "2020-11-22 17:49:04.659766: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-11-22 17:49:04.681700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\n",
      "2020-11-22 17:49:04.681926: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5afea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-22 17:49:04.681941: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-22 17:49:04.682029: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "train_data_filenames ['/opt/ml/input/data/train/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/train/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['/opt/ml/input/data/train/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/train/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "WARNING:tensorflow:From /opt/ml/code/train.py:80: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "**************** train *****************\n",
      "{'input_ids': array([[  101,  2092,  1012, ...,  1012,  2034,   102],\n",
      "       [  101,  1045,  7868, ...,     0,     0,     0],\n",
      "       [  101,  1045,  2031, ...,  3340,  1012,   102],\n",
      "       ...,\n",
      "       [  101, 22817,  2023, ...,     0,     0,     0],\n",
      "       [  101,  2023,  2097, ...,  6097,  1012,   102],\n",
      "       [  101,  4007,  2573, ...,  2185,  2007,   102]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]]), 'label_ids': array([0, 2, 2, 4, 1, 4, 3, 3, 0, 4, 1, 0, 2, 4, 0, 4, 0, 2, 3, 0, 0, 2,\n",
      "       0, 4, 1, 1, 3, 0, 1, 3, 4, 0, 3, 4, 4, 2, 4, 2, 0, 2, 4, 3, 3, 2,\n",
      "       4, 2, 4, 2, 4, 4, 3, 3, 4, 4, 2, 4, 3, 2, 0, 1, 0, 4, 3, 4, 1, 3,\n",
      "       3, 0, 2, 2, 4, 1, 4, 3, 3, 0, 4, 1, 0, 2, 4, 0, 4, 0, 2, 3, 0, 0,\n",
      "       2, 0, 4, 1, 1, 3, 0, 1, 3, 4, 0, 3, 4, 4, 2, 4, 2, 0, 2, 4, 3, 3,\n",
      "       2, 4, 2, 4, 2, 4, 4, 3, 3, 4, 4, 2, 4, 3, 2, 0, 1, 0]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.06MB/s]\n",
      "Downloading: 100%|██████████| 442/442 [00:00<00:00, 477kB/s]\n",
      "Downloading: 100%|██████████| 363M/363M [00:17<00:00, 21.2MB/s] \n",
      "2020-11-22 17:49:29.927568: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Sucessfully downloaded after 0 retries.\n",
      "** use_amp False\n",
      "*** OPTIMIZER <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f0124303400> ***\n",
      "Compiled model <transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x7f0124303a58>\n",
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,957,317\n",
      "Trainable params: 594,437\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n",
      "None\n",
      "validation_data_filenames ['/opt/ml/input/data/validation/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/validation/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['/opt/ml/input/data/validation/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord', '/opt/ml/input/data/validation/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "**************** validation *****************\n",
      "{'input_ids': array([[ 101, 9733, 2001, ..., 1998, 4149,  102],\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 2042, 2478, ...,    0,    0,    0],\n",
      "       [ 101, 2017, 2031, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2572, ...,    0,    0,    0]]), 'input_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3,\n",
      "       1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2,\n",
      "       2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2, 2, 3, 1, 2]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "Starting Training and Validation...\n",
      "Train for 100 steps, validate for 10 steps\n",
      "Epoch 1/3\n",
      "2020-11-22 17:49:51.280088: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1574] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "100/100 [==============================] - 348s 3s/step - loss: 1.5589 - accuracy: 0.3015 - val_loss: 1.7256 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 332s 3s/step - loss: 1.4774 - accuracy: 0.3970 - val_loss: 1.8293 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 333s 3s/step - loss: 1.4151 - accuracy: 0.4724 - val_loss: 1.8989 - val_accuracy: 0.0000e+00\n",
      "2020-11-22 18:06:39.165811: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "<tensorflow.python.keras.callbacks.History object at 0x7f00dc176b70>\n",
      "test_data_filenames ['/opt/ml/input/data/test/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord', '/opt/ml/input/data/test/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord']\n",
      "***** Using input_filenames ['/opt/ml/input/data/test/part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord', '/opt/ml/input/data/test/part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord']\n",
      "**************** test *****************\n",
      "{'input_ids': array([[  101,  2023, 11989, ...,     0,     0,     0],\n",
      "       [  101,  1996,  8816, ...,     0,     0,     0],\n",
      "       [  101,  6581,  4031, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1996,  8816, ...,     0,     0,     0],\n",
      "       [  101,  6581,  4031, ...,     0,     0,     0],\n",
      "       [  101,  2069,  2138, ...,     0,     0,     0]]), 'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label_ids': array([0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0,\n",
      "       4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3,\n",
      "       0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3, 0, 0, 4, 3]), 'segment_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])}\n",
      "Starting test...\n",
      "10/10 [==============================] - 21s 2s/step - loss: 1.6077 - accuracy: 0.2500\n",
      "2020-11-22 18:07:04.524242: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "Test history [1.6077237129211426, 0.25]\n",
      "transformer_fine_tuned_model_path /opt/ml/model/transformers/fine-tuned/\n",
      "INFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/transformers/fine-tuned/config.json\n",
      "tensorflow_saved_model_path /opt/ml/model/tensorflow/saved_model/0\n",
      "INFO:transformers.modeling_tf_utils:Model weights saved in /opt/ml/model/transformers/fine-tuned/tf_model.h5\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f0124303048>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f0124303048>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01243882b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01243882b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f012437d3c8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f012437d3c8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01278b6b70>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01278b6b70>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01243e8b38>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01243e8b38>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01240cc8d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f01240cc8d0>, because it is not built.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /opt/ml/model/tensorflow/saved_model/0/assets\n",
      "INFO:tensorflow:Assets written to: /opt/ml/model/tensorflow/saved_model/0/assets\n",
      "INFO:transformers.configuration_utils:loading configuration file /opt/ml/model/transformers/fine-tuned/config.json\n",
      "INFO:transformers.configuration_utils:Model config DistilBertConfig {\n",
      "  \"_num_labels\": 5,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": 1,\n",
      "    \"1\": 2,\n",
      "    \"2\": 3,\n",
      "    \"3\": 4,\n",
      "    \"4\": 5\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"1\": 0,\n",
      "    \"2\": 1,\n",
      "    \"3\": 2,\n",
      "    \"4\": 3,\n",
      "    \"5\": 4\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_tf_utils:loading weights file /opt/ml/model/transformers/fine-tuned/tf_model.h5\n",
      "INFO:transformers.modeling_tf_utils:Layers of TFDistilBertForSequenceClassification not initialized from pretrained model: ['dropout_39']\n",
      "INFO:transformers.modeling_tf_utils:Layers from pretrained model not used in TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "inference_device -1\n",
      "I loved it!  I will recommend this to everyone. [{'label': 5, 'score': 0.27236706}]\n",
      "It's OK. [{'label': 1, 'score': 0.24923496}]\n",
      "Really bad.  I hope they don't make this anymore. [{'label': 1, 'score': 0.2524342}]\n",
      "CPU times: user 7.64 s, sys: 2.21 s, total: 9.85 s\n",
      "Wall time: 17min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!kubectl logs -f bert-model-training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
